{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_draft.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chaohua95/IBM-data-science-Capstone/blob/master/MLP_draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Di7w8SvzySEu",
        "colab": {}
      },
      "source": [
        "#import necessary libraries \n",
        "import h5py \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oIBsPJpwyqz_",
        "colab": {}
      },
      "source": [
        "N_classes= 10\n",
        "N_input= 128\n",
        "# define funciton for reading training data\n",
        "def read_training_set(): \n",
        "    # read training data file\n",
        "    with h5py.File('DataSet/train_128.h5', 'r') as H: \n",
        "        data = np.copy(H['data']) \n",
        "    # read training labels file\n",
        "    with h5py.File('DataSet/train_label.h5', 'r') as H: \n",
        "        labels = np.copy(H['label']) \n",
        "    \n",
        "    # convert labels to one-hot verctors \n",
        "    label_vectors = np.zeros((labels.shape[0], N_classes))\n",
        "    for i in range(0, labels.shape[0]):\n",
        "        for j in range(0, N_classes):\n",
        "            if labels[i,] == j:\n",
        "                label_vectors[i, j] = 1    \n",
        "    return data, label_vectors\n",
        "\n",
        "# define funciton for reading test data\n",
        "def read_test_data():\n",
        "    # read test data file\n",
        "    with h5py.File('../Input/test_128.h5', 'r') as H:\n",
        "        data = np.copy(H['data'])\n",
        "    return data\n",
        "\n",
        "# define function to normalise the data\n",
        "def normalise(data_set):\n",
        "    data_size = data_set.shape[0]\n",
        "    feature_size = data_set.shape[1]\n",
        "    \n",
        "    means = data_set.mean(axis=0) #calculate means of each feature \n",
        "    stds = data_set.std(axis=0) #calculate standard deviation of each feature\n",
        "    \n",
        "    data_normalised = (data_set - means)/stds \n",
        "            \n",
        "    return data_normalised"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y6hiEpmOBTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, train_labels = read_training_set()\n",
        "train_data = normalise(train_data)\n",
        "\n",
        "\n",
        "# train_data= train_data[0:1000,:]\n",
        "# train_labels= train_labels[0:1000,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vobviGKdOBTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Activation(object):\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def tanh_deriv(self, a):\n",
        "#         a = np.tanh(x)   \n",
        "        return 1.0 - a**2\n",
        "    \n",
        "    def logistic(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    def logistic_deriv(self, a):\n",
        "        # a = logistic(x) \n",
        "        return  a * (1 - a )\n",
        "   \n",
        "    def relu(self, x):\n",
        "        return (x > 0)*x\n",
        "    \n",
        "    def relu_deriv(self, a):\n",
        "        return (a > 0)\n",
        "    \n",
        "    def softmax(self, x):\n",
        "        return np.exp(x)/sum(np.exp(x))\n",
        "    \n",
        "    def softmax_deriv(self, a):\n",
        "        # a = softmax(x) \n",
        "        return  a \n",
        "\n",
        "    \n",
        "    def __init__(self,activation='tanh'):\n",
        "        if activation == 'logistic':\n",
        "            self.f = self.logistic\n",
        "            self.f_deriv = self.logistic_deriv\n",
        "        elif activation == 'tanh':\n",
        "            self.f = self.tanh\n",
        "            self.f_deriv = self.tanh_deriv\n",
        "        elif activation == 'relu':\n",
        "            self.f = self.relu\n",
        "            self.f_deriv = self.relu_deriv\n",
        "        elif activation == 'softmax':\n",
        "            self.f = self.softmax\n",
        "            self.f_deriv = self.softmax_deriv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G1rr-iyOBTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HiddenLayer(object):    \n",
        "    def __init__(self,n_in, n_out,\n",
        "                 activation_last_layer='tanh',activation='tanh', W=None, b=None):\n",
        "  \n",
        "        self.input=None\n",
        "        self.activation= Activation(activation).f\n",
        "        \n",
        "        self.activation_deriv=None\n",
        "        if activation_last_layer: #There's another layer before, not input layer\n",
        "            self.activation_deriv= Activation(activation_last_layer).f_deriv #used to pass the delta on\n",
        "\n",
        "        #initialize weights\n",
        "        self.W = np.random.uniform(\n",
        "                low=-np.sqrt(6. / (n_in + n_out)),\n",
        "                high=np.sqrt(6. / (n_in + n_out)),\n",
        "                size=(n_in, n_out)\n",
        "        )\n",
        "        if activation == 'logistic':\n",
        "            self.W *= 4\n",
        "\n",
        "        self.b = np.zeros(n_out,)\n",
        "        \n",
        "        self.grad_W = np.zeros(self.W.shape)\n",
        "        self.grad_b = np.zeros(self.b.shape)\n",
        "        self.acc_grad_W= np.zeros(self.W.shape)\n",
        "        self.acc_grad_b= np.zeros(self.b.shape)\n",
        "        self.n_in= n_in\n",
        "        self.n_out= n_out\n",
        "        self.mask= np.ones(n_out,)\n",
        "\n",
        "    #unified function calls\n",
        "    def forward(self, input, dropout, train, output_layer= False): \n",
        "        lin_output = np.dot(input, self.W) + self.b\n",
        "        self.output = (\n",
        "            lin_output if self.activation is None\n",
        "            else self.activation(lin_output)\n",
        "        )\n",
        "        if train and dropout and not output_layer and self.activation_deriv:\n",
        "            self.mask= (np.random.binomial(1, (1-dropout), self.n_in)/(1-dropout))\n",
        "#             self.output *= self.mask \n",
        "            input = input * self.mask\n",
        "        self.input= input\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, delta, learning_rate, momentum, decay, dropout, output_layer=False): \n",
        "#         if dropout and not output_layer and self.activation_deriv:\n",
        "#             delta= delta * self.mask \n",
        "        #(optimize=\"sgd\")\n",
        "        # Gradient= delta * Input\n",
        "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
        "        self.grad_b = delta\n",
        "        if decay: #weight decay\n",
        "            self.grad_W += decay*self.W\n",
        "        if self.activation_deriv: #There's another layer before, pass the delta on\n",
        "            #Next_delta = delta * W * Deriv(layer Input)\n",
        "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input) #next delta for layer before\n",
        "        \n",
        "        if dropout and not output_layer and self.activation_deriv:\n",
        "            delta= delta * self.mask\n",
        "\n",
        "        if momentum:\n",
        "            #(optimize=\"momentum\") One-sample sgd or mini-batch\n",
        "            self.acc_grad_W = momentum*self.acc_grad_W - learning_rate*self.grad_W\n",
        "            self.acc_grad_b = momentum*self.acc_grad_b - learning_rate*self.grad_b         \n",
        "        else:\n",
        "            #(optimize=\"sgd\",batch_size >1) Mini-batch\n",
        "            self.acc_grad_W = self.acc_grad_W - learning_rate*self.grad_W\n",
        "            self.acc_grad_b = self.acc_grad_b - learning_rate*self.grad_b\n",
        "        return delta\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8zWQkX7OBTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP:     \n",
        "    def __init__(self, layers, activation=[None,'tanh','tanh']):\n",
        "        ### initialize layers\n",
        "        self.layers=[]\n",
        "        self.params=[]\n",
        "        \n",
        "        self.activation= activation\n",
        "        for i in range(len(layers)-1):\n",
        "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1])) \n",
        "    \n",
        "    def forward(self,input, dropout=0, train= False):\n",
        "        output_layer= False\n",
        "        for layer in self.layers:\n",
        "            if layer == self.layers[-1]: output_layer= True\n",
        "            output=layer.forward(input, dropout, train, output_layer)\n",
        "            input=output\n",
        "        return output\n",
        "    \n",
        "    def MSE(self,y,y_hat,decay):                \n",
        "        activation_deriv= Activation(self.activation[-1]).f_deriv        \n",
        "        error = y_hat-y #mse\n",
        "        l2= decay*(self.layers[-1].W**2).sum() / 2 \n",
        "        loss= (error**2).sum()  +  l2\n",
        "        delta= error*activation_deriv(y_hat)  # calculate delta of the output layer\n",
        "        return loss,delta\n",
        "    \n",
        "    def crossEntropy(self,y,y_hat,decay):\n",
        "        #softmax\n",
        "        if round(sum(y_hat)) != 1.:\n",
        "            y_hat= np.exp(y_hat)/sum(np.exp(y_hat))\n",
        "        #cross Entropy    \n",
        "        activation_deriv= Activation(self.activation[-1]).f_deriv\n",
        "        l2= decay*(self.layers[-1].W**2).sum() / 2\n",
        "        loss= -1*np.dot(y, np.log(y_hat+1e-7)) + l2\n",
        "        error= y_hat-y #with softmax\n",
        "        delta= error # calculate delta of the output layer\n",
        "        return loss,delta   \n",
        "        \n",
        "    def backward(self,delta, learning_rate, momentum, decay, dropout):\n",
        "        delta=self.layers[-1].backward(delta, learning_rate, momentum, decay, dropout, output_layer=True)#back propagate outer delta\n",
        "        for layer in reversed(self.layers[:-1]):\n",
        "            delta=layer.backward(delta, learning_rate, momentum, decay, dropout)#back propagate delta\n",
        "            \n",
        "    def update(self,lr, momentum, batch_size):\n",
        "        if not momentum:\n",
        "            #(optimize=\"sgd\")\n",
        "            for layer in self.layers:\n",
        "                layer.W -= lr * layer.grad_W\n",
        "                layer.b -= lr * layer.grad_b\n",
        "        else:\n",
        "            #(optimize=\"momentum\") One-sample sgd or mini-batch\n",
        "            for layer in self.layers:\n",
        "                layer.W += layer.acc_grad_W\n",
        "                layer.b += layer.acc_grad_b\n",
        "            #mini-batch reset (batch_size > 1)\n",
        "            if batch_size > 1:\n",
        "                for layer in self.layers:\n",
        "                    layer.acc_grad_W = np.zeros(layer.acc_grad_W.shape)\n",
        "                    layer.acc_grad_b= np.zeros(layer.acc_grad_b.shape)\n",
        "    \n",
        "    def predict(self, x): \n",
        "        x = np.array(x)\n",
        "        output = []\n",
        "        for i in np.arange(x.shape[0]):\n",
        "            output.append(self.forward(x[i,:]))\n",
        "        return np.array(output)\n",
        "    \n",
        "    def fit(self,X,y, optimize= \"sgd\", regularize= None, cost= \"mse\", dropout= 0.5,\n",
        "            decay= 0.001, batch_size= 1, learning_rate=0.1, momentum= 0.9, epochs=250):\n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "        to_return = np.zeros(epochs)  \n",
        "        if optimize != \"momentum\": #sgd \n",
        "            momentum= None\n",
        "        if regularize !=\"wd\":\n",
        "            decay= 0\n",
        "        if regularize != \"dropout\":\n",
        "            dropout= 0\n",
        "            \n",
        "        for k in range(epochs):\n",
        "            loss=np.zeros(X.shape[0])\n",
        "            \n",
        "            #shuffle index------------------------------------------------\n",
        "            index_shuf = list(range(X.shape[0]))\n",
        "            np.random.shuffle(index_shuf)\n",
        "            n_batches = X.shape[0] // batch_size\n",
        "            j= 0\n",
        "            for i in range(n_batches): \n",
        "                X_batch= X[index_shuf[i * batch_size:(i + 1)*batch_size], :]\n",
        "                y_batch= y[index_shuf[i * batch_size:(i + 1)*batch_size], :]\n",
        "                \n",
        "                #inside batch---------------------------------------------\n",
        "                for i2 in range(batch_size):\n",
        "                    # forward pass\n",
        "                    y_hat = self.forward(X_batch[i2,:], dropout, train= True)\n",
        "                    if cost==\"crossentropy\":                    \n",
        "                        loss[j],delta=self.crossEntropy(y_batch[i2,:],y_hat,decay)\n",
        "                        if(np.isnan(loss[j])): return to_return[0:k-1]\n",
        "                    else:\n",
        "                        loss[j],delta=self.MSE(y_batch[i2,:],y_hat,decay)\n",
        "                        if(np.isnan(loss[j])): return to_return[0:k-1]\n",
        "                    j+=1\n",
        "                    # backward pass    \n",
        "                    self.backward(delta, learning_rate, momentum, decay, dropout)\n",
        "                    \n",
        "                # update--------------------------------------------------\n",
        "                self.update(learning_rate, momentum, batch_size)\n",
        "            to_return[k] = np.mean(loss)  \n",
        "            print('loss at epoch ' + str(k) + \": \" + str(to_return[k]))\n",
        "        return to_return    \n",
        "       \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqCjEW3WOBTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Try different MLP models\n",
        "nn = MLP([N_input, 64, 32, 16, N_classes], [None,'relu', 'relu', 'relu', 'softmax'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "S6bOyrEYOBTx",
        "colab_type": "code",
        "colab": {},
        "outputId": "de134b14-f917-4503-bd47-505d6b1673da"
      },
      "source": [
        "### Try different learning rate and epochs\n",
        "MSE = nn.fit(train_data,train_labels, optimize= \"momentum\", cost= \"crossentropy\",  regularize= \"dropout\", dropout= 0.1,\n",
        "             batch_size= 200, learning_rate=0.01, epochs=50)\n",
        "print('loss:%f'%MSE[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss at epoch 0: 1.306962728142269\n",
            "loss at epoch 1: 0.6223788034869322\n",
            "loss at epoch 2: 0.5294123420893421\n",
            "loss at epoch 3: 0.4868005905859126\n",
            "loss at epoch 4: 0.4629800978071907\n",
            "loss at epoch 5: 0.4416570742677548\n",
            "loss at epoch 6: 0.424252223087295\n",
            "loss at epoch 7: 0.41218474212895717\n",
            "loss at epoch 8: 0.40459305974403287\n",
            "loss at epoch 9: 0.39299041883555286\n",
            "loss at epoch 10: 0.39135640766679886\n",
            "loss at epoch 11: 0.3816350107056029\n",
            "loss at epoch 12: 0.379046244824506\n",
            "loss at epoch 13: 0.3799642890590786\n",
            "loss at epoch 14: 0.373706473213852\n",
            "loss at epoch 15: 0.3653861906985298\n",
            "loss at epoch 16: 0.3705965200792302\n",
            "loss at epoch 17: 0.36924765263360654\n",
            "loss at epoch 18: 0.34942801394922707\n",
            "loss at epoch 19: 0.3473971593530204\n",
            "loss at epoch 20: 0.3431924556390626\n",
            "loss at epoch 21: 0.3478143789838454\n",
            "loss at epoch 22: 0.3432238064317335\n",
            "loss at epoch 23: 0.3369587127674719\n",
            "loss at epoch 24: 0.3328370604658605\n",
            "loss at epoch 25: 0.32950972301642134\n",
            "loss at epoch 26: 0.3273344970270294\n",
            "loss at epoch 27: 0.32343266989861846\n",
            "loss at epoch 28: 0.3278100493373873\n",
            "loss at epoch 29: 0.3250706348901003\n",
            "loss at epoch 30: 0.3203206883629935\n",
            "loss at epoch 31: 0.31745273493147746\n",
            "loss at epoch 32: 0.32717645150569685\n",
            "loss at epoch 33: 0.3196917590830594\n",
            "loss at epoch 34: 0.3181226235612829\n",
            "loss at epoch 35: 0.31496218756369765\n",
            "loss at epoch 36: 0.31296063443305444\n",
            "loss at epoch 37: 0.3026035590998213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3h9D_z7OBTz",
        "colab_type": "code",
        "colab": {},
        "outputId": "5367c660-3bc7-4ec8-b88b-2df5a7427d7b"
      },
      "source": [
        "plt.figure(figsize=(15,4))\n",
        "plt.plot(MSE)\n",
        "plt.grid()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIauQCMdOBT1",
        "colab_type": "code",
        "colab": {},
        "outputId": "7bec604f-517a-47f7-ba5c-790ca816ba60"
      },
      "source": [
        "train_labels2= np.argmax(train_labels, axis=1)\n",
        "output = nn.predict(train_data)\n",
        "output= np.argmax(output, axis=1)\n",
        "print(sum(output==train_labels2)/len(output))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8229166666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp_zNmlAOBT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tru_Nw38OBT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}